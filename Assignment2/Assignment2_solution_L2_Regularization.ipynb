{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWvDL80f8khB",
        "outputId": "5f0ffe66-b561-44ce-9c51-6c4cf787c376"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow import keras\n",
        "\n",
        "np.random.seed(9630)\n",
        "tf.random.set_seed(9630)\n",
        "\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "goNTLx1Y8vMV",
        "outputId": "30531fb5-2bd8-4881-a2c4-af1d7be4c495"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST or FMNIST\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data() \n",
        "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "assert X_train.shape == (60000, 28, 28)\n",
        "assert X_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)\n",
        "\n",
        "\n",
        "# Display randomly selected data\n",
        "indices = list(np.random.randint(X_train.shape[0],size=3))\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.imshow(X_train[indices[i]].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Index {} Class {}\".format(indices[i], y_train[indices[i]]))\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        },
        "id": "hZzR1OmDCZh4",
        "outputId": "ad0a3da5-a2d6-4584-e435-a4be68211abd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train dataset into train and validation\n",
        "X_val = X_train[50000:60000]\n",
        "X_train = X_train[0:50000]\n",
        "y_val = y_train[50000:60000]\n",
        "y_train = y_train[0:50000]\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example is\", str(X_train.shape[1]), \"by\", str(X_train.shape[2]))\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example is\", str(X_val.shape[1]), \"by\", str(X_val.shape[2]))\n",
        "\n",
        "X_train = X_train.reshape(50000, 28*28)\n",
        "X_val = X_val.reshape(10000, 28*28)\n",
        "X_test = X_test.reshape(10000, 28*28)\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example has\", str(X_train.shape[1]), \"features\")\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example has\", str(X_val.shape[1]), \"features\")\n",
        "\n",
        "# Split dataset into batches\n",
        "#train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(16)\n",
        "#test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYA1r_xRCbXK",
        "outputId": "dfb75dbd-7049-4720-f801-5ba7d0c86798"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of training set is 50000 samples\n",
            "every train example is 28 by 28\n",
            "size of validation set is 10000 samples\n",
            "every validation example is 28 by 28\n",
            "size of training set is 50000 samples\n",
            "every train example has 784 features\n",
            "size of validation set is 10000 samples\n",
            "every validation example has 784 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize Data\n",
        "\n",
        "X_train = X_train/255\n",
        "X_val = X_val/255\n",
        "X_test = X_test/255\n",
        "# X_train[0]\n",
        "np.max(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzPNDEVdCe5m",
        "outputId": "44d3ec5b-ff79-4382-98e2-47d09f1d02e1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size_input = X_train.shape[1]\n",
        "size_hidden = 128\n",
        "size_output = 10\n",
        "\n",
        "number_of_train_examples = X_train.shape[0]\n",
        "number_of_test_examples = X_test.shape[0]\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10) # Other function is tf.one_hot(y_train,depth=10)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "print(tf.shape(y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ5NVG2jCgqk",
        "outputId": "a5a02d70-8d80-4dd4-babc-dbc685638dd9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([10000    10], shape=(2,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Th5Cfbv58053"
      },
      "outputs": [],
      "source": [
        "# def mnist_normalize():\n",
        "#   (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "#   X_train = np.reshape(X_train, (X_train.shape[0], 784)) / 255.\n",
        "#   X_test = np.reshape(X_test, (X_test.shape[0], 784)) / 255.\n",
        "#   y_train = tf.keras.utils.to_categorical(y_train)\n",
        "#   y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "#   return (X_train, y_train), (X_test, y_test)\n",
        "\n",
        "# def fashion_normalize():\n",
        "#   (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "#   X_train = np.reshape(X_train, (X_train.shape[0], 784)) / 255.\n",
        "#   X_test = np.reshape(X_test, (X_test.shape[0], 784)) / 255.\n",
        "#   y_train = tf.keras.utils.to_categorical(y_train)\n",
        "#   y_test = tf.keras.utils.to_categorical(y_test)\n",
        "\n",
        "#   return (X_train, y_train), (X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FUcIZ_rF83L7"
      },
      "outputs": [],
      "source": [
        "# # Split dataset into batches\n",
        "# # (X_train, y_train), (X_test, y_test) = mnist_normalize()\n",
        "# (X_train, y_train), (X_test, y_test) = fashion_normalize()\n",
        "\n",
        "# train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(16)\n",
        "# test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oSeWjfQo85Vh"
      },
      "outputs": [],
      "source": [
        "# Define class to build MLP model\n",
        "class MLP(object):\n",
        "  def __init__(self, size_input, size_hidden, size_output, device = None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden: int, size of hidden layer (2)\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. \n",
        "    If None, the device will automatically decide during Eager Execution ('gpu')\n",
        "    \"\"\"\n",
        "\n",
        "    self.size_input, self.size_hidden, self.size_output, self.device =\\\n",
        "    size_input, size_hidden, size_output, device\n",
        "\n",
        "    # Initialize weights of input layer\n",
        "    self.W0 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden], stddev=0.1))\n",
        "    # Initialize biases for input layer\n",
        "    self.b0 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "\n",
        "    # Initialize weights between input layer and hidden layer 1\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden], stddev=0.1))\n",
        "    # Initialize biases for hidden layer 1\n",
        "    self.b1 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "\n",
        "    # Initialize weights between hidden layer 1 and hidden layer 2\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden, self.size_hidden], stddev=0.1))\n",
        "    # Initialize biases for hidden layer 2\n",
        "    self.b2 = tf.Variable(tf.random.normal([1, self.size_hidden]))\n",
        "\n",
        "    # Initialize weights between hidden layer 2 and output layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden, self.size_output], stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b3 = tf.Variable(tf.random.normal([1, self.size_output]))\n",
        "\n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W0, self.W1, self.W2, self.W3, self.b0, self.b1, self.b2, self.b3]\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "    return self.y\n",
        "\n",
        "  def loss(self, y_pred, y_true):\n",
        "    \"\"\"\n",
        "    y_pred: Tensor of shape (batch_size, size_output)\n",
        "    y_true: Tensor of shape (batch_size, size_output)\n",
        "    \"\"\"\n",
        "\n",
        "    y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    return tf.losses.mean_squared_error(y_true_tf, y_pred_tf)\n",
        "\n",
        "  def loss_with_regularization(self, y_pred, y_true, lambd = 0.7):\n",
        "    m = y_true.shape[1]\n",
        "    W0 = self.variables[0]\n",
        "    W1 = self.variables[1]\n",
        "    W2 = self.variables[2]\n",
        "    W3 = self.variables[3]\n",
        "\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      pre_loss = self.loss(predicted, y_train)\n",
        "    # L2_regularization_loss = lambd * (np.sum(np.square(W0)) + np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) / (2 * m)\n",
        "    L2_regularization_loss = lambd * (np.sum(np.square(W1)) + np.sum(np.square(W2)) + np.sum(np.square(W3))) / (2 * m)\n",
        "    final_loss = pre_loss + L2_regularization_loss\n",
        "\n",
        "    return final_loss\n",
        "\n",
        "  def backward_with_regularization(self, X_train, y_train):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "\n",
        "    optimizer = tf.keras.optimizers.SGD(learning_rate = 3e-5)\n",
        "    with tf.GradientTape() as tape:\n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss_with_regularization(predicted, y_train)\n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        "  def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "\n",
        "    # Cast X to float 32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    # Remember to normalize your dataset before moving forward\n",
        "\n",
        "    # Compute values in input layer\n",
        "    what0 = tf.matmul(X_tf, self.W0) + self.b0\n",
        "    hhat0 = tf.nn.relu(what0)\n",
        "\n",
        "    # Compute values in hidden layer 1\n",
        "    what1 = tf.matmul(what0, self.W1) + self.b1\n",
        "    hhat1 = tf.nn.relu(what1)\n",
        "\n",
        "    # Compute values in hidden layer 2\n",
        "    what2 = tf.matmul(hhat1, self.W2) + self.b2\n",
        "    hhat2 = tf.nn.relu(what2)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(hhat2, self.W3) + self.b3\n",
        "    output = tf.nn.softmax(output)\n",
        "\n",
        "    # Now consider two things\n",
        "    # First, look at inbuild loss functions if they work with softmax and then change this\n",
        "    # Second, add tf.Softmax(output) and then return this variable\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofZ8bduA9ACl"
      },
      "source": [
        "#Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "1YNS-5rm8-KN"
      },
      "outputs": [],
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6EXBcqhq9CfU"
      },
      "outputs": [],
      "source": [
        "# def compute_acc(y_pred, y_true):\n",
        "#   maxid = lambda x: np.argmax(x)\n",
        "#   y_pred_max = np.array([maxid(i) for i in y_pred])\n",
        "#   y_true_max = np.array([maxid(j) for j in y_true])\n",
        "#   acc = sum(y_pred_max == y_true_max) / len(y_pred_max)\n",
        "#   return acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def compute_std(y_pred):\n",
        "#   y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "#   std_dev = np.std(y_pred_tf)\n",
        "#   std_err = std_dev / np.sqrt(len(y_pred_tf))\n",
        "#   variance = std_dev**2\n",
        "#   return std_err, variance"
      ],
      "metadata": {
        "id": "k7gc2jZRs22o"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emYJFPPz9E-e",
        "outputId": "a46b15e9-99bd-43c4-bb81-0d45d21f82c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8786200284957886\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 15542.83264 \n",
            "\n",
            "Validation Accuracy: 0.8618\n",
            "\n",
            "Train Accuracy: 0.8996000289916992\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 16493.66784 \n",
            "\n",
            "Validation Accuracy: 0.8750\n",
            "\n",
            "Train Accuracy: 0.9140999913215637\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 17282.14016 \n",
            "\n",
            "Validation Accuracy: 0.8795\n"
          ]
        }
      ],
      "source": [
        "# Initialize model using GPU\n",
        "mlp_on_gpu = MLP(size_input, size_hidden, size_output, device = 'gpu')\n",
        "time_start = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "\n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(9630)).batch(40)\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_gpu.forward(inputs)\n",
        "    loss_total = loss_total + mlp_on_gpu.loss_with_regularization(preds, outputs)\n",
        "    mlp_on_gpu.backward_with_regularization(inputs, outputs)\n",
        "\n",
        "  preds = mlp_on_gpu.forward(X_train)\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "\n",
        "  preds_val = mlp_on_gpu.forward(X_val)\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, 'float'))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        " \n",
        "time_taken = time.time() - time_start\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "# For per epoch_time\n",
        "# epoch_time = Total time / Number of epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5-xBFUc9GgZ"
      },
      "source": [
        "#Test Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQO42uif9HN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7003f75-451f-4831-f402-de9987af4bb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test MSE: 0.0260\n",
            "Accuracy: 0.8694\n"
          ]
        }
      ],
      "source": [
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(4)\n",
        "\n",
        "\n",
        "for inputs, outputs in test_ds:\n",
        "  preds = mlp_on_gpu.forward(inputs)\n",
        "  # b = mlp_on_gpu.loss(preds, outputs)\n",
        "  test_loss_total = test_loss_total + mlp_on_gpu.loss(preds, outputs)\n",
        "print('Test loss: {:.4f}'.format(np.sum(test_loss_total.numpy()) / X_test.shape[0]))\n",
        "\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_gpu.forward(X_test)\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Dhka44BelfvU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "Assignment2_solution_L2_Regularization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}