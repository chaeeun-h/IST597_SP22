{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment3_1,2,3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(9630)\n",
        "tf.random.set_seed(9630)"
      ],
      "metadata": {
        "id": "mtbzbtWez1mp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHg7yZc60CKk",
        "outputId": "43077232-5ae7-4f0a-e17c-95da0303b24f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST or FMNIST\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data() \n",
        "# (X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data() \n",
        "assert X_train.shape == (60000, 28, 28)\n",
        "assert X_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)\n",
        "\n",
        "\n",
        "# Display randomly selected data\n",
        "indices = list(np.random.randint(X_train.shape[0],size=3))\n",
        "for i in range(3):\n",
        "    plt.subplot(1,3,i+1)\n",
        "    plt.imshow(X_train[indices[i]].reshape(28,28), cmap='gray', interpolation='none')\n",
        "    plt.title(\"Index {} Class {}\".format(indices[i], y_train[indices[i]]))\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "4IDf1hvZ0Gbt",
        "outputId": "ae86759b-7503-42f9-9a0f-751862d585be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train dataset into train and validation\n",
        "X_val = X_train[50000:60000]\n",
        "X_train = X_train[0:50000]\n",
        "y_val = y_train[50000:60000]\n",
        "y_train = y_train[0:50000]\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example is\", str(X_train.shape[1]), \"by\", str(X_train.shape[2]))\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example is\", str(X_val.shape[1]), \"by\", str(X_val.shape[2]))\n",
        "\n",
        "X_train = X_train.reshape(50000, 28*28)\n",
        "X_val = X_val.reshape(10000, 28*28)\n",
        "X_test = X_test.reshape(10000, 28*28)\n",
        "\n",
        "print(\"size of training set is\", str(X_train.shape[0]), \"samples\")\n",
        "print(\"every train example has\", str(X_train.shape[1]), \"features\")\n",
        "\n",
        "print(\"size of validation set is\", str(X_val.shape[0]), \"samples\")\n",
        "print(\"every validation example has\", str(X_val.shape[1]), \"features\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFbQhmwA0NTD",
        "outputId": "b87c42ad-ad5a-409e-df2a-68b1489eed7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "size of training set is 50000 samples\n",
            "every train example is 28 by 28\n",
            "size of validation set is 10000 samples\n",
            "every validation example is 28 by 28\n",
            "size of training set is 50000 samples\n",
            "every train example has 784 features\n",
            "size of validation set is 10000 samples\n",
            "every validation example has 784 features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize Data\n",
        "\n",
        "X_train = X_train/255\n",
        "X_val = X_val/255\n",
        "X_test = X_test/255\n",
        "# X_train[0]\n",
        "np.max(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDmqSYSf0QuK",
        "outputId": "6bec29c0-b19e-4263-c605-edf14928b695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size_input = X_train.shape[1]\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 128\n",
        "size_hidden3 = 128\n",
        "size_output = 10\n",
        "\n",
        "number_of_train_examples = X_train.shape[0]\n",
        "number_of_test_examples = X_test.shape[0]\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10) # Other function is tf.one_hot(y_train,depth=10)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "print(tf.shape(y_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFNHcu6Z0TKE",
        "outputId": "d73e839d-d69c-4fd5-cff9-7b62d3938c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([10000    10], shape=(2,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4]\n",
        "  \n",
        " def forward(self, X):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X)\n",
        "    else:\n",
        "      self.y = self.compute_output(X)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, y_train, opti):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = opti\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "           \n",
        " def compute_output(self, X):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "    h1 = tf.nn.relu(z1)\n",
        "    h1 = tf.nn.dropout(h1, rate = dropout_p, seed = 9630)\n",
        "    \n",
        "    z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "    h2 = tf.nn.relu(z2)\n",
        "    h2 = tf.nn.dropout(h2, rate = dropout_p, seed = 9630)\n",
        "    \n",
        "    z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "    h3 = tf.nn.relu(z3)\n",
        "    h3 = tf.nn.dropout(h3, rate = dropout_p, seed = 9630)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    \n",
        "    #Now consider two things , First look at inbuild loss functions if they work with softmax or not and then change this \n",
        "    # Second add tf.Softmax(output) and then return this variable\n",
        "    return (output)\n",
        "\n",
        "#  def stderr(self,y_pred):\n",
        "#     \"\"\"\n",
        "#      Calculate standard error\n",
        "#      \"\"\"\n",
        "#     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "#     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "#     std_err = std_dev/sqrt(len(y_pred_tf))\n",
        "#     return std_err \n",
        "\n",
        "\n",
        "#  def var(self,y_pred):\n",
        "#     \"\"\"\n",
        "#      Calculate variance \n",
        "#      \"\"\"\n",
        "#     y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "#     std_dev = np.std(y_pred_tf) #Calculates standard deviation\n",
        "#     variance = (std_dev**2) # calculate variance\n",
        "#     return variance "
      ],
      "metadata": {
        "id": "OAmtM8_T0VUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SGD"
      ],
      "metadata": {
        "id": "K0ZksAAk0fI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 Trials for Stability Test\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "dropout_p = 0.0\n",
        "\n",
        "seeds = [9630]\n",
        "# seeds = np.random.randint(1000,9999,10)\n",
        "test_acc_total = []\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "opti = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "time_start_total = time.time()\n",
        "\n",
        "for i in range(len(seeds)):\n",
        "  time_start = time.time()\n",
        "  seed = seeds[i]\n",
        "  print('======================= Trial:', i+1, '=======================')\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "      \n",
        "    loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "    lt = 0\n",
        "      \n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(128)\n",
        "    kz = 0\n",
        "    accuracy_z = 0.0\n",
        "    cur_train_acc = 0.0\n",
        "    for inputs, outputs in train_ds:\n",
        "      qw, tr = tf.shape(inputs)\n",
        "      kz = kz + 1\n",
        "      preds = mlp_on_cpu.forward(0, inputs) \n",
        "      loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "      lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "      mlp_on_cpu.backward(inputs, outputs, opti)\n",
        "\n",
        "    preds = mlp_on_cpu.forward(X_train)\n",
        "    # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "    preds = tf.nn.softmax(preds)\n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "    accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    cur_train_acc += accuracy_z.numpy()\n",
        "    ds = cur_train_acc\n",
        "    print('\\nTrain Accuracy: {}'.format(ds))\n",
        "    print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "    \n",
        "    preds_val = mlp_on_cpu.forward(X_val)\n",
        "    preds_val = tf.nn.softmax(preds_val)\n",
        "    correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    cur_val_acc = accuracy.numpy()\n",
        "\n",
        "    print('\\nValidation Accuracy: {}'.format(cur_val_acc))\n",
        "    \n",
        "    plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "  plt.show()        \n",
        "  time_taken = time.time() - time_start\n",
        "      \n",
        "  # Validate model\n",
        "      \n",
        "  # Test Model\n",
        "  preds = mlp_on_cpu.forward(0, X_test)\n",
        "  pred = np.argmax(preds, axis = 1)\n",
        "  y_true = np.argmax(y_test, axis = 1)\n",
        "  test_acc = (pred==y_true).mean()\n",
        "  test_acc_total.append(test_acc)\n",
        "\n",
        "  print('\\nTest Accuracy = {}'.format(test_acc))\n",
        "\n",
        "  print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "  #For per epoch_time = Total_Time / Number_of_epochs\n",
        "\n",
        "print('\\nTotal time take (in seconds): {:.2f}'.format(time.time() - time_start_total))\n",
        "print('Test Accuracy', test_acc_total)\n",
        "plt_x = [1,2,3,4,5,6,7,8,9,10]\n",
        "plt.ylim(0.7, 1.0)\n",
        "plt.plot(plt_x, test_acc_total, 'ro-')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "s7gTyxK6Ggkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMSprop"
      ],
      "metadata": {
        "id": "WAtWcGED0myP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 Trials for Stability Test\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "dropout_p = 0.0\n",
        "\n",
        "seeds = [9630]\n",
        "# seeds = np.random.randint(1000,9999,10)\n",
        "test_acc_total = []\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "opti = tf.keras.optimizers.RMSprop(learning_rate = 1e-3)\n",
        "time_start_total = time.time()\n",
        "\n",
        "for i in range(len(seeds)):\n",
        "  time_start = time.time()\n",
        "  seed = seeds[i]\n",
        "  print('======================= Trial:', i+1, '=======================')\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "      \n",
        "    loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "    lt = 0\n",
        "      \n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(128)\n",
        "    kz = 0\n",
        "    accuracy_z = 0.0\n",
        "    cur_train_acc = 0.0\n",
        "    for inputs, outputs in train_ds:\n",
        "      qw, tr = tf.shape(inputs)\n",
        "      kz = kz + 1\n",
        "      preds = mlp_on_cpu.forward(0, inputs) \n",
        "      loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "      lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "      mlp_on_cpu.backward(inputs, outputs, opti)\n",
        "\n",
        "    preds = mlp_on_cpu.forward(X_train)\n",
        "    # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "    preds = tf.nn.softmax(preds)\n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "    accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    cur_train_acc += accuracy_z.numpy()\n",
        "    ds = cur_train_acc\n",
        "    print('\\nTrain Accuracy: {}'.format(ds))\n",
        "    print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "    \n",
        "    preds_val = mlp_on_cpu.forward(X_val)\n",
        "    preds_val = tf.nn.softmax(preds_val)\n",
        "    correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    cur_val_acc = accuracy.numpy()\n",
        "\n",
        "    print('\\nValidation Accuracy: {}'.format(cur_val_acc))\n",
        "    \n",
        "    plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "  plt.show()        \n",
        "  time_taken = time.time() - time_start\n",
        "      \n",
        "  # Validate model\n",
        "      \n",
        "  # Test Model\n",
        "  preds = mlp_on_cpu.forward(0, X_test)\n",
        "  pred = np.argmax(preds, axis = 1)\n",
        "  y_true = np.argmax(y_test, axis = 1)\n",
        "  test_acc = (pred==y_true).mean()\n",
        "  test_acc_total.append(test_acc)\n",
        "\n",
        "  print('\\nTest Accuracy = {}'.format(test_acc))\n",
        "\n",
        "  print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "  #For per epoch_time = Total_Time / Number_of_epochs\n",
        "\n",
        "print('\\nTotal time take (in seconds): {:.2f}'.format(time.time() - time_start_total))\n",
        "print('Test Accuracy', test_acc_total)\n",
        "plt_x = [1,2,3,4,5,6,7,8,9,10]\n",
        "plt.ylim(0.7, 1.0)\n",
        "plt.plot(plt_x, test_acc_total, 'ro-')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6KWOBrxjHjPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adam"
      ],
      "metadata": {
        "id": "rzJ4sFNS0zwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10 Trials for Stability Test\n",
        "# Set number of epochs\n",
        "NUM_EPOCHS = 20\n",
        "dropout_p = 0.0\n",
        "\n",
        "seeds = [9630]\n",
        "# seeds = np.random.randint(1000,9999,10)\n",
        "test_acc_total = []\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "opti = tf.keras.optimizers.Adam(learning_rate = 1e-4)\n",
        "time_start_total = time.time()\n",
        "\n",
        "for i in range(len(seeds)):\n",
        "  time_start = time.time()\n",
        "  seed = seeds[i]\n",
        "  print('======================= Trial:', i+1, '=======================')\n",
        "  for epoch in range(NUM_EPOCHS):\n",
        "      \n",
        "    loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "    lt = 0\n",
        "      \n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(seed)).batch(128)\n",
        "    kz = 0\n",
        "    accuracy_z = 0.0\n",
        "    cur_train_acc = 0.0\n",
        "    for inputs, outputs in train_ds:\n",
        "      qw, tr = tf.shape(inputs)\n",
        "      kz = kz + 1\n",
        "      preds = mlp_on_cpu.forward(0, inputs) \n",
        "      loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "      lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "      mlp_on_cpu.backward(inputs, outputs, opti)\n",
        "\n",
        "    preds = mlp_on_cpu.forward(X_train)\n",
        "    # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "    preds = tf.nn.softmax(preds)\n",
        "    correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "    accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    cur_train_acc += accuracy_z.numpy()\n",
        "    ds = cur_train_acc\n",
        "    print('\\nTrain Accuracy: {}'.format(ds))\n",
        "    print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "    \n",
        "    preds_val = mlp_on_cpu.forward(X_val)\n",
        "    preds_val = tf.nn.softmax(preds_val)\n",
        "    correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "    cur_val_acc = accuracy.numpy()\n",
        "\n",
        "    print('\\nValidation Accuracy: {}'.format(cur_val_acc))\n",
        "    \n",
        "    plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "  plt.show()        \n",
        "  time_taken = time.time() - time_start\n",
        "      \n",
        "  # Validate model\n",
        "      \n",
        "  # Test Model\n",
        "  preds = mlp_on_cpu.forward(0, X_test)\n",
        "  pred = np.argmax(preds, axis = 1)\n",
        "  y_true = np.argmax(y_test, axis = 1)\n",
        "  test_acc = (pred==y_true).mean()\n",
        "  test_acc_total.append(test_acc)\n",
        "\n",
        "  print('\\nTest Accuracy = {}'.format(test_acc))\n",
        "\n",
        "  print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "  #For per epoch_time = Total_Time / Number_of_epochs\n",
        "\n",
        "print('\\nTotal time take (in seconds): {:.2f}'.format(time.time() - time_start_total))\n",
        "print('Test Accuracy', test_acc_total)\n",
        "plt_x = [1,2,3,4,5,6,7,8,9,10]\n",
        "plt.ylim(0.7, 1.0)\n",
        "plt.plot(plt_x, test_acc_total, 'ro-')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fhUI5ZFG27-4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}